# Database
DATABASE_URL=postgresql://user:password@localhost:5432/weft

# Server
PORT=3001
NODE_ENV=development

# Better Auth
# A secret value used for encryption and hashing (min 32 chars)
# Generate with: openssl rand -base64 32
BETTER_AUTH_SECRET=your-secret-key-here-change-this-in-production

# The base URL of your application (used for callbacks, redirects, etc.)
BETTER_AUTH_URL=http://localhost:3001

# Optionally set the app URL for the frontend
BETTER_AUTH_APP_URL=http://localhost:3000

# OAuth Providers (uncomment to enable)
# GITHUB_CLIENT_ID=your_github_client_id
# GITHUB_CLIENT_SECRET=your_github_client_secret
# GOOGLE_CLIENT_ID=your_google_client_id
# GOOGLE_CLIENT_SECRET=your_google_client_secret

# Email (for password reset and verification)
# SMTP_HOST=smtp.example.com
# SMTP_PORT=587
# SMTP_USER=your_email@example.com
# SMTP_PASSWORD=your_email_password
# SMTP_FROM=noreply@example.com

# Transcription Configuration
TRANSCRIPTION_MODEL=Xenova/whisper-small.en  # Options: tiny, base, small, medium, large
TRANSCRIPTION_WORKER_CONCURRENCY=2  # Lower than API version (local is CPU-intensive)
TRANSCRIPTION_MAX_RETRIES=3
TRANSCRIPTION_RETRY_DELAY_MS=5000
TRANSCRIPTION_TIMEOUT_MS=600000  # 10 minutes (local is slower than API)

# Whisper Models Directory (where downloaded models are stored)
# Defaults to /app/uploads/whisper-models if not set
# WHISPER_MODELS_DIR=/app/uploads/whisper-models

# FFmpeg Path (if not in system PATH)
# FFMPEG_PATH=/usr/local/bin/ffmpeg

# ============================================
# MASTRA AI FRAMEWORK CONFIGURATION
# ============================================

# Provider Selection: "ollama" or "openrouter"
MASTRA_PROVIDER=ollama

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
# Enable thinking mode for models that support it (e.g., Qwen3)
# Set to 'false' to disable thinking mode
# NOTE: With Ollama + Qwen3, thinking mode may cause the agent loop to stop
# after tool calls. If you experience this, try setting to 'false'.
OLLAMA_THINKING=true
# Maximum agent steps for tool calls + response (default: 10)
OLLAMA_MAX_STEPS=10
# Use non-streaming mode for Ollama (may help with tool calling issues)
# Set to 'true' to use generate() instead of stream()
OLLAMA_DISABLE_STREAMING=false

# OpenRouter Configuration (for cloud models)
# OPENROUTER_API_KEY=your-openrouter-api-key-here
# OPENROUTER_MODEL=anthropic/claude-sonnet-4

# Default model to use (fallback)
# Format: gateway/provider/model (e.g., ollama/ollama/qwen3:30b)
MASTRA_DEFAULT_MODEL=ollama/llama3.2:3b
